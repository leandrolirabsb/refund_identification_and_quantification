{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVATION !!!! The content of part of this public code was removed in order to preserve\n",
    "                    # informations about the Anatel database structure and to preserve sensitive data\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import pyodbc\n",
    "import nltk \n",
    "import sklearn\n",
    "!pip install stanza\n",
    "import stanza\n",
    "import re\n",
    "!pip install -U gensim\n",
    "import gensim\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import time\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import randint as sp_randint\n",
    "!pip install spacy\n",
    "import spacy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.stem import RSLPStemmer\n",
    "!pip install pyodbc \n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('floresta')\n",
    "stanza.download('pt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    return sentence\n",
    "\n",
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase\n",
    "\n",
    "#construction a lemmantization using stanza toolkit\n",
    "nlp = stanza.Pipeline('pt')\n",
    "\n",
    "def Lemmatize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    lemma = \"\"\n",
    "    for sent in nlp(sentence).sentences:\n",
    "        for word in sent.words:\n",
    "            lemma += word.lemma + \"\\t\"\n",
    "\n",
    "    return lemma\n",
    "\n",
    "#Function that lemmatize the text, tokenize it and than return the list of  tokenized Lemmas \n",
    "def LemmatizeTokenized(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    lemmas = [word.lemma for sent in nlp(sentence).sentences for word in sent.words]\n",
    "    return lemmas\n",
    "\n",
    "stops = set(nltk.corpus.stopwords.words('portuguese') + \n",
    "            ['protocolo','reclamada','prestadora','operadora','operador','prestador','empresa',\n",
    "            'oi','vivo','tim','claro','embratel', 'sky','algar','telefonica','telefônica','net','nextel',\n",
    "            'dia','dias','data','ano', 'anos','após','fez','referente','consumidor','consumidora','para',\n",
    "            'pois','porém','anatel','reclama','reclamação','reclamar','providência','pede','telefônico',\n",
    "            'providências','janeiro','fevereiro','março','abril','maio','junho','julho','agosto',\n",
    "            'setembro','outubro','novembro','dezembro','providencia','providencias','telefone','contrato','contratar',\n",
    "            'ser','fiz','ter','sendo','diz','disse','nada','nome','vir','www',\n",
    "            'numero','número','nr','nº','vc','vcs','pq','tb','ai','aí', '(a)','(es)','!', '?','mês',\n",
    "             '(',')','*','-','2019','2020', '2021','\\\"',\"\\'\", '@', '&','#'])\n",
    "\n",
    "#obs.: LEMBRAR de montar expressão regular para remover: numeros de protocolo, números de telefone e datas\n",
    "def RemoveStopWords(sentence):\n",
    "    sentence = Tokenize(sentence)\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        if word not in stops:\n",
    "            phrase.append(word)\n",
    "    return phrase\n",
    "\n",
    "def RemoveStopWordsAndLemmatize(sentence):\n",
    "    #sentence = sentence.lower() we expect receive a sentence already in lower case\n",
    "    lemmas = \"\"\n",
    "    for sent in nlp(sentence).sentences:\n",
    "        for word in sent.words:\n",
    "            if(word.lemma not in stops): lemmas += word.lemma + \" \"\n",
    "\n",
    "    return lemmas\n",
    "\n",
    "def LemmatizeSentence(sentence):\n",
    "    #sentence = sentence.lower() we expect receive a sentence already in lower case\n",
    "    lemmas = \"\"\n",
    "    for sent in nlp(sentence).sentences:\n",
    "        for word in sent.words:\n",
    "            lemmas += word.lemma + \" \"\n",
    "\n",
    "    return lemmas\n",
    "\n",
    "#fraseTeste = Tokenize(\"Bom dia! Vamos testar este tokenizador Jóia!\")\n",
    "fraseTeste = Tokenize(\"Vasco da Gama é o maior português de sempre!\")\n",
    "\n",
    "print(fraseTeste)\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "print(Stemming(fraseTeste))\n",
    "\n",
    "sentenceWordsPointRefund = (\"ressarcimento ressarcida \"+\n",
    "                            \"ressarcido ressarcir ressarça \" +\n",
    "                            \"restituição restituicao restitua restituir reembolso reembolse reembolsar reembolsado \" +\n",
    "                            \"indenização indenize indenizacao indenizado \")\n",
    "\n",
    "#describe the words that point a refund by the company\n",
    "sentenceWordsPointRefundCompany = (\"lançamento lancamento desconto reajuste reajustamos devolucao devolvemos devolução \" +\n",
    "                            \"devolvido devolvida ressarcimento ressarcida ajuste ajustado ajustada liberado \"+\n",
    "                            \"bônus bonus \" +\n",
    "                            \"crédito credito créditos creditos creditar \" +\n",
    "                            \"isentado isentar\")\n",
    "\n",
    "#words that can point to a refund. Let's introduce a lemmatization process\n",
    "setWordsRefound = Tokenize(sentenceWordsPointRefund)\n",
    "print(setWordsRefound)\n",
    "\n",
    "#stemming words point to a refund\n",
    "stemmedWordsRefund = Stemming(setWordsRefound)\n",
    "print(stemmedWordsRefund)\n",
    "\n",
    "#lemmatize words to a refund\n",
    "lemmatizedWordsRefund = Tokenize(Lemmatize(sentenceWordsPointRefund))\n",
    "\n",
    "\n",
    "setLemmatizedWordsRefund = set(['ressarcimento','ressarcidar', 'ressarcir', 'ressarçar', 'restituição',\n",
    "                             'restituicao', 'restituir',  'reembolso', 'reembolse', 'reembolsar',\n",
    "                             'indenização', 'indenizar', 'indenizacao', 'estorno', 'estornar', 'extorno',\n",
    "                              'extornar', 'extorne', 'estorne','devolução','devolucao','devolver', 'crédito','credito',\n",
    "                              'creditar', 'compensação','compensar','reparação', 'reparar','reaver', 'multar','fidelidade',\n",
    "                              'valor','absurdo', 'desconto', 'redução','roubo', 'preço','negociar','debito', 'juro',\n",
    "                              'caro','contar','indevidamente', 'indevido','isenção'])\n",
    "\n",
    "setLemmatizedWordsRefundCompany = set(['lançar','abatimento','isento','isenção','conceder','baixo','acordo','desconto',\n",
    "                                       'credito','renovação','ajuste','contestar','lançar','ressarcir','corrigir','diferença',\n",
    "                                       'readequação','desconsiderar','concessão', 'resolver','baixo','cancelamento',\n",
    "                                       'providenciar'])\n",
    "\n",
    "setLemmasPointSum = set(['de','e'])\n",
    "\n",
    "setLemmasPointSubtraction = set(['para'])\n",
    "\n",
    "\n",
    "#words that can point to a refund in the answers. Let's introduce a lemmatization process\n",
    "setWordsRefundCompany = Tokenize(sentenceWordsPointRefundCompany)\n",
    "print(setWordsRefundCompany)\n",
    "\n",
    "#stemming words point to a refund in a company answer\n",
    "stemmedWordsRefundCompany = Stemming(setWordsRefundCompany)\n",
    "print(setWordsRefundCompany)\n",
    "\n",
    "#lemmatize words to a refund in a company answer\n",
    "lemmatizedWordsRefundCompany = Tokenize(Lemmatize(sentenceWordsPointRefundCompany))\n",
    "\n",
    "\n",
    "\n",
    "#function to check if there is some word that point a refund inside the complaint.\n",
    "def checkForRefundWords(complaint):\n",
    "    tokensComplaint = Tokenize(complaint)\n",
    "    stemmedWordsComplaint = Stemming(tokensComplaint)\n",
    "    \n",
    "    for steamInComplaint in stemmedWordsComplaint:\n",
    "        if steamInComplaint in stemmedWordsRefund:\n",
    "            print(\"Stemming detected \"+steamInComplaint)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def checkForRefundWordsUsingLemma(complaint):\n",
    "    lemmatizedWordsComplaint = Lemmatize(complaint)\n",
    "    tokensComplaint = Tokenize(lemmatizedWordsComplaint)\n",
    "    \n",
    "    for lemmaInComplaint in tokensComplaint:\n",
    "        if lemmaInComplaint in lemmatizedWordsRefund:\n",
    "            print(\"Lemma detected \"+lemmaInComplaint)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def checkForRefundWordsUsingLemma2(complaint):\n",
    "    tokensComplaint = LemmatizeTokenized(complaint)\n",
    "    \n",
    "    for lemmaInComplaint in tokensComplaint:\n",
    "        if lemmaInComplaint in setLemmatizedWordsRefund:\n",
    "            print(\"Lemma detected \"+lemmaInComplaint)\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "#function to check if there is some word that point a refund inside the COMPANY ANSWER.\n",
    "def checkForRefundWordCompany(answers):\n",
    "    tokensAnswers = Tokenize(answers)\n",
    "    stemmedWordsAnswers= Stemming(tokensAnswers)\n",
    "    \n",
    "    for steamInComplaint in stemmedWordsAnswers:\n",
    "        if steamInAnswer in stemmedWordsRefundCompany:\n",
    "            print(\"Stemming answer detected \"+steamInAnswer)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def checkForRefundWordsCompanyUsingLemma(answers):\n",
    "    lemmatizedWordsAnswers = Lemmatize(answers)\n",
    "    tokensAnswers = Tokenize(lemmatizedWordsAnswers)\n",
    "    \n",
    "    for lemmaInAnswer in tokensAnswers:\n",
    "        if lemmaInAnswer in lemmatizedWordsRefundCompany:\n",
    "            print(\"Lemma answer detected \"+lemmaInAnswer)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def checkForRefundWordsCompanyUsingLemma2(answers):\n",
    "    tokensAnswers = LemmatizeTokenized(answers)\n",
    "    \n",
    "    for lemmaInAnswer in tokensAnswers:\n",
    "        if lemmaInAnswer in setLemmatizedWordsRefundCompany:\n",
    "            print(\"Lemma answer detected \"+lemmaInAnswer)\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def preprocess_text(textComplaint, includeStopWords):\n",
    "       \n",
    "    textComplaint = textComplaint.lower()\n",
    "    \n",
    "    #first we remove string that represent protocols number, dates and telephone numbers\n",
    "    #removing protocol numbers\n",
    "    p1 = re.compile('\\d{15}') #removind protocol numbers with 15 digits\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{14}') #removind protocol numbers with 14 digits\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{13}') #removind protocol numbers with 13 digits\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{2}/\\d{2}/\\d{4}') #removind dates: format dd/mm/yyyy\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "                        \n",
    "    p1 = re.compile('\\d{2}/\\d{2}') #removind short dates: format dd/mm\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{2}h\\d{2}') #removind times: format examples 14h35  \n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{2}:\\d{2}') #removind times: format examples 14:35 \n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{1}h\\d{2}') #removind times: format examples 9h35  \n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{1}:\\d{2}') #removind times: format examples 9:35 \n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\(\\d{2}\\)') #removind telephone number area code: format (11)\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{2}\\s\\d{9}')   # remoing numbers like Example 21 985883880\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{2}\\s\\d{8}')   # remoing numbers like Example 21 985883880\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{7}-\\d{4}')   # remoing numbers like Example 2197033-4392\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{6}-\\d{4}')   # removing numbers like Example 213321-4392\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{7}\\s\\d{4}')   # removing numbers like Example 2197033 4392\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{6}\\s\\d{4}')   # removing numbers like Example 213321 4392\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{2}\\s\\d{5}-\\d{4}')   # removing numbers like Example 31 98131-3131\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{2}\\s\\d{4}-\\d{4}')   # removing numbers like Example 31 3321-3131\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    \n",
    "    p1 = re.compile('\\d{2}\\s\\d{5}\\s\\d{4}')   # removing numbers like Example 31 98131 3131\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{2}\\s\\d{4}\\s\\d{4}')   # removing numbers like Example 31 3321 3131\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    \n",
    "    p1 = re.compile('\\d{11}|\\d{10}|\\d{9}|\\d{8}') #removind telephone numbers 8 to 11 digits: format 11982165069\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "\n",
    "      \n",
    "    p1 = re.compile('\\d{5}-\\d{4}') #removind telephone numbers like format 99991-0621\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "\n",
    "    p1 = re.compile('\\d{4}-\\d{4}') #removind telephone numbers like format 9991-0621\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "    \n",
    "    p1 = re.compile('\\d{5}\\s\\d{4}') #removind telephone numbers like format 99991 0621\n",
    "    textComplaint = p1.sub('', textComplaint)\n",
    "\n",
    "    p1 = re.compile('\\d{4}\\s\\d{4}') #removind telephone numbers like format 9991 0621\n",
    "    textComplaint = p1.sub('', textComplaint)   \n",
    "    \n",
    "    #than we lemmatize\n",
    "    #textLemmatized = Lemmatize(textComplaint)\n",
    "    \n",
    "    #than we remove stop words over the lemmatized text. Inside the stop words function there is a tokenization and lower case\n",
    "    lst_text = \"\"\n",
    "    if(includeStopWords): lst_text = LemmatizeSentence(textComplaint) #case where we don`t remove stop words\n",
    "    else: lst_text = RemoveStopWordsAndLemmatize(textComplaint) # case where we remove stop words\n",
    "    \n",
    "    #than we return the new string\n",
    "    ## back to string from list\n",
    "    #text = \" \".join(lst_text)\n",
    "    #return text\n",
    "    return lst_text\n",
    "\n",
    "#function that combine the complaint and the answer, and then \n",
    "def getPreprocessFullComplaint(textComplaint, answers):\n",
    "    fullComplaint = str(textComplaint) + \" \"+ str(answers)\n",
    "    return preprocess_text(fullComplaint,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVATION !!!! The content of this function was removed from this public code\n",
    "                    #in order to preserve informations about the Anatel database structure\n",
    "\n",
    "#lets introduce here the analysis of the company response, in order to introduce in the train and test dataframes\n",
    "# the column containing the answers to the complaint\n",
    "#function to obtains the complaint answers given an specific complaint part of the answer\n",
    "# we will chek also if the part of the answer is not equal to the complaint. If it is, we do not include in answers\n",
    "def returnComplaintAnswers(idComplaint, textComplaint):\n",
    "    complaintAnswers = \"\"\n",
    "   \n",
    "    #OBSERVATION !!!! The content of this function was removed from this public code\n",
    "                    #in order to preserve informations about the Anatel database structure\n",
    "    \n",
    "    return complaintAnswers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init callback class\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        \n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        elif self.epoch % 100 == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        \n",
    "        \n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init word2vec class\n",
    "w2v_model = Word2Vec(vector_size = 300, #The number of dimensions of the embedding\n",
    "                     window = 10, #The maximum distance between a target word and words around the target word.\n",
    "                     min_count = 2, #The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "                     workers = 16, #The number of threads to use while training\n",
    "                     sg = 1, #The training algorithm, either CBOW (0) or skip gram (1)\n",
    "                     negative = 5,\n",
    "                     sample = 1e-5)\n",
    "# build vovab\n",
    "w2v_model.build_vocab(tokenized_corpus)\n",
    "\n",
    "# train the w2v model\n",
    "start = time.time()\n",
    "w2v_model.train(tokenized_corpus, \n",
    "                total_examples=w2v_model.corpus_count, \n",
    "                epochs=1001, \n",
    "                report_delay=1,\n",
    "                compute_loss = True, # set compute_loss = True\n",
    "                callbacks=[callback()]) # add the callback class\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(\"elapsedtime in seconds to train the embedding model :\"+ str(end - start))\n",
    "# save the word2vec model\n",
    "w2v_model.save('C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_w2v_model = Word2Vec.load('C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/word2vec.model')\n",
    "\n",
    "words = list(reloaded_w2v_model.wv.key_to_index)\n",
    "print('Vocab size: '+str(len(words)))\n",
    "w1 = 'valor'\n",
    "print(\"Top 3 words similar to valor:\",\\\n",
    "      reloaded_w2v_model.wv.most_similar(positive = w1,topn =3))\n",
    "w1 = 'restituição'\n",
    "print(\"Top 3 words similar to restituição:\",\\\n",
    "      reloaded_w2v_model.wv.most_similar(positive = w1,topn =3))\n",
    "print(\"Similarity between reembolso and valor:\"+\\\n",
    "      str(reloaded_w2v_model.wv.similarity(w1=\"reembolso\",w2=\"valor\")))\n",
    "print(\"Similarity between ressarcimento and indignar:\"+\\\n",
    "      str(reloaded_w2v_model.wv.similarity(w1=\"ressarcimento\",w2=\"indignar\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    for word in model.wv.key_to_index:\n",
    "        vectors.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    #labels = np.asarray(labels)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(reloaded_w2v_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    \n",
    "    \n",
    "    indices = list(range(len(labels)))\n",
    "    #selected_indices = random.sample(indices, 25)\n",
    "    selected_indices=[]\n",
    "    index = labels.index(\"reembolso\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"quantia\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"valor\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"dinheiro\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"abuso\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"devolução\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"crédito\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"ressarcimento\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"indenização\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"restituição\")\n",
    "    selected_indices.append(index)\n",
    "    \n",
    "    \n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "\n",
    "\n",
    "plot_function = plot_with_matplotlib\n",
    "\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#function to vectorize sentences\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.asarray(sent_vec) / numw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the vectors over the pre-process (cleaned) text\n",
    "STATIC_FLAG_FULL_COMPLAINT = 0\n",
    "STATIC_FLAG_COMPLAINT = 1\n",
    "STATIC_FLAG_ANSWERS = 2\n",
    "model_gensim = reloaded_w2v_model\n",
    "def createDocsVectors(dataFrame, optionText):\n",
    "\n",
    "    #testing new approach\n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "    if(optionText == STATIC_FLAG_FULL_COMPLAINT):textComplaints = dataFrame['TEXT_CLEAN_FULL_COMPLAINT'].values\n",
    "    if(optionText == STATIC_FLAG_COMPLAINT):textComplaints = dataFrame['TEXT_CLEAN_COMPLAINT'].values\n",
    "    if(optionText == STATIC_FLAG_ANSWERS):textComplaints = dataFrame['TEXT_CLEAN_ANSWERS'].values\n",
    "\n",
    "    for text in textComplaints: # looping through each document and cleaning it\n",
    "        temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "        for word in text.split(' '): # looping through each word of a single document and spliting through space\n",
    "                try:\n",
    "                    #word_vec = embeddings[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                    word_vec = sent_vectorizer(word, model_gensim)\n",
    "                    temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "                except:\n",
    "                    pass\n",
    "        doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
    "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "    return docs_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizing the heuristic using special words. Here we creaete dataframes to store the results of the heuristic\n",
    "COLUNAS = [\n",
    "    'HAS_WORD_POINT_REFUND',\n",
    "    'HAS_WORD_POINT_REFUND_AGREE'\n",
    "]\n",
    "\n",
    "dataFrameHeuristicTrain = pd.DataFrame(columns=COLUNAS)\n",
    "dataFrameHeuristicTest = pd.DataFrame(columns=COLUNAS)\n",
    "\n",
    "dataFrameTrain = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/dataFrameTrain1200PreProcessed.csv', sep='\\t')\n",
    "dataFrameTest = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/dataFrameTest.csv', sep='\\t')\n",
    "\n",
    "dataFrameHeuristicTrain['HAS_WORD_POINT_REFUND'] = dataFrameTrain['TEXT_CLEAN_COMPLAINT'].apply(checkForRefundWordsUsingLemma2)\n",
    "dataFrameHeuristicTrain['HAS_WORD_POINT_REFUND_AGREE'] = dataFrameTrain['TEXT_CLEAN_ANSWERS'].apply(checkForRefundWordsCompanyUsingLemma2)\n",
    "\n",
    "dataFrameHeuristicTrain.to_csv('C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/dataFrameHeuristicTrain.csv', sep='\\t', index=False)\n",
    "\n",
    "dataFrameHeuristicTest['HAS_WORD_POINT_REFUND'] = dataFrameTest['TEXT_CLEAN_COMPLAINT'].apply(checkForRefundWordsUsingLemma2)\n",
    "dataFrameHeuristicTest['HAS_WORD_POINT_REFUND_AGREE'] = dataFrameTest['TEXT_CLEAN_ANSWERS'].apply(checkForRefundWordsCompanyUsingLemma2)\n",
    "dataFrameHeuristicTest.to_csv('C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/dataFrameHeuristicTest.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets show the accuracy of the heuristic over  COMPLAINTS\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "#y1 = dataFrame['INCLUDE_REFUND'].values #case train\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for Sample of '+ str(len(dataFrame)) +' COMPLAINTS using Just special words: ')\n",
    "accuracy = accuracy_score(y1,dataFrameHeuristic['HAS_WORD_POINT_REFUND'])\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y1,dataFrameHeuristic['HAS_WORD_POINT_REFUND']))\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(y1, dataFrameHeuristic['HAS_WORD_POINT_REFUND'])\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(y1, dataFrameHeuristic['HAS_WORD_POINT_REFUND'])\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(y1, dataFrameHeuristic['HAS_WORD_POINT_REFUND'])\n",
    "print('Precision score: %f' % precision)\n",
    "print('\\n')\n",
    "\n",
    "#lets show the accuracy of the heuristic over 1200 ANSWERS\n",
    "#y2 = dataFrame['REFUND_AGREE'].values#case train\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for Sample of '+ str(len(dataFrame)) +' ANSWERS using Just special words: ')\n",
    "accuracy = accuracy_score(y2,dataFrameHeuristic['HAS_WORD_POINT_REFUND_AGREE'])\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y2,dataFrameHeuristic['HAS_WORD_POINT_REFUND_AGREE']))\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(y2, dataFrameHeuristic['HAS_WORD_POINT_REFUND_AGREE'])\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(y2, dataFrameHeuristic['HAS_WORD_POINT_REFUND_AGREE'])\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(y2, dataFrameHeuristic['HAS_WORD_POINT_REFUND_AGREE'])\n",
    "print('Precision score: %f' % precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting cases where there is a complaint for refund in the ID's below\n",
    "#functin to tag the complains that includ an asking for refund \n",
    "def tagRefund(id):\n",
    "    return taggedFrames.loc[id]['CLASSIFICACAO']\n",
    "\n",
    "\n",
    "#functin to tag the complains that includ a custumer asking for refund. Here we consult the by hand tagged dataFrame(taggedFrames)\n",
    "def tagRefundAsked(id):\n",
    "    return taggedFrames.loc[id]['REEMBOLSO_SOLICITADO']\n",
    "\n",
    "#functin to inlcude the refund value complained by the customers, based on the id. Here we consult\n",
    "# the by hand tagged dataFrame(taggedFrames)\n",
    "def tagValueComplainded(id):\n",
    "     return taggedFrames.loc[id]['VALOR_SOLICITADO']\n",
    "\n",
    "#functin to tag the complains that includ a company's agreement for refund. Here we consult the by hand tagged dataFrame(taggedFrames)\n",
    "def tagRefundAgree(id):\n",
    "    return taggedFrames.loc[id]['REEMBOLSO_CONCEDIDO']\n",
    "\n",
    "#functin to inlcude the value refunded by the companies, based on the id. Here we consult\n",
    "# the by hand tagged dataFrame(taggedFrames)\n",
    "def tagValueRefunded(id):\n",
    "     return taggedFrames.loc[id]['VALOR_CONCEDIDO']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataFrame['INCLUDE_REFUND'] = dataFrame['ID'].apply(tagRefund)    \n",
    "\n",
    "#here, we include a new column in the dataFrame, containing if the customer asked for a refund\n",
    "#dataFrame['REFUND_ASKED'] = dataFrame['ID'].apply(tagRefundAsked)    \n",
    "\n",
    "#here, we include a new column in the dataFrame, containing the value complained for refund\n",
    "#dataFrame['REAL_VALUE_COMPLAINED'] = dataFrame['ID'].apply(tagValueComplainded)    \n",
    "\n",
    "#here, we include a new column in the dataFrame, containing the answers to the complaint \n",
    "#dataFrame['ANSWERS'] = dataFrame.apply(lambda x: returnComplaintAnswers(x['ID'],x['REGISTRODESCRICAO']), axis=1) \n",
    "\n",
    "#here, we include a new column in the dataFrame, containing the information IF the company has offered some refund\n",
    "dataFrame['REFUND_AGREE'] = dataFrame['ID'].apply(tagRefundAgree)    \n",
    "\n",
    "#here, we include a new column in the dataFrame, containing the value the company has refunded\n",
    "#dataFrame['REAL_VALUE_REFUNDED'] = dataFrame['ID'].apply(tagValueRefunded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors1200Complaints.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(docs_vectors.drop('INCLUDE_REFUND', axis = 1),\n",
    "                                                   docs_vectors['INCLUDE_REFUND'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=800, random_state = 1)\n",
    "model.fit(train_x, train_y)\n",
    "test_pred = model.predict(test_x)\n",
    "\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the AdaBoostClassifier in TRAIN sample of '+str(len(dataFrame)) +' complaints: ')\n",
    "accuracy = accuracy_score(test_y, test_pred)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y,test_pred))\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(test_y,test_pred)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred)\n",
    "print('Precision score: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we apply the classification model over the test sample of 400 complaints\n",
    "docs_vectorsTest = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors400TestComplaints.csv', sep='\\t')\n",
    "test_x = docs_vectorsTest\n",
    "test_y = taggedDataFrameTest['REEMBOLSO_SOLICITADO'].values\n",
    "test_pred = model.predict(test_x)\n",
    "\n",
    "\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the AdaBoostClassifier in TEST sample of '+str(len(test_x)) +' complaints: ')\n",
    "accuracy = accuracy_score(test_y, test_pred)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y,test_pred))\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(test_y,test_pred)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred)\n",
    "print('Precision score: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#functions definition to obtain the accuracy (exact match), FP, FN, TN  TP from values predictions \n",
    "def checkMach(realValue, predictedValue):\n",
    "    if realValue == predictedValue:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def checkFalsePositive(realValue, predictedValue):\n",
    "    if ( (realValue == 0) and (predictedValue !=0) ):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def checkFalseNegative(realValue, predictedValue):\n",
    "    if ( (realValue != 0) and (predictedValue ==0) ):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def checkTruePositive(realValue, predictedValue):\n",
    "    if ( (realValue != 0) and (predictedValue !=0) ):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def checkTrueNegative(realValue, predictedValue):\n",
    "    if ( (realValue == 0) and (predictedValue ==0) ):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "real_V = 8.3\n",
    "pred_V = 8.3\n",
    "match = checkMach(real_V, pred_V)\n",
    "falsePositive = checkFalsePositive(real_V, pred_V)\n",
    "falseNegative = checkFalseNegative(real_V, pred_V)\n",
    "truePositive = checkTruePositive(real_V, pred_V)\n",
    "trueNegative = checkTrueNegative(real_V, pred_V)\n",
    "\n",
    "print('Real Value: %f' % real_V)\n",
    "print('Predicted Value: %f' % pred_V)\n",
    "print('Mach score: %f' % match)\n",
    "print('False Positive: %f' % falsePositive)\n",
    "print('False Negative: %f' % falseNegative)\n",
    "print('True Positive: %f' % truePositive)\n",
    "print('True Negative: %f' % trueNegative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, for the complaints that has been predictec as containing an asking for refund, we will include the values complained\n",
    "dataFrameTest = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/dataFrameTest.csv', sep='\\t')\n",
    "#predictionsIncludeAskForRefund = test_pred\n",
    "\n",
    "#dataFrameTest['TEXT_CLEAN_COMPLAINT_WITH_STOP_WORDS'] = dataFrameTest.apply(lambda x: preprocess_text(x['REGISTRODESCRICAO'],True), axis=1)\n",
    "#dataFrameTest['TEXT_CLEAN_ANSWERS_WITH_STOP_WORDS'] = dataFrameTest.apply(lambda x: preprocess_text(x['ANSWERS'],True), axis=1)\n",
    "\n",
    "\n",
    "dataFrameTest2 = dataFrameTest \n",
    "#including in the test data frame the predictions if is a compaint that includes asking for refund\n",
    "dataFrameTest2['REAL_VALUE_COMPLAINED'] = taggedDataFrameTest['VALOR_SOLICITADO'] # the true values\n",
    "dataFrameTest2['ESTIMATED_REFUND_COMPLAINT'] =  taggedDataFrameTest['REEMBOLSO_SOLICITADO'] # include just the true values, to focus this task just in get the value\n",
    "\n",
    "#in this context, we will use as \"predicted values\" for cases that involves some Customer ask for complaint the true values\n",
    "predictionsIncludeAskForRefund = taggedDataFrameTest['REEMBOLSO_SOLICITADO']\n",
    "\n",
    "#including in the test data frame the predictions for asked refund values\n",
    "#dataFrameTest2['ESTIMATED_VALUE_COMPLAINED'] = dataFrameTest2.apply(lambda x: getRefundAskedValueInComplaint(x['TEXT_CLEAN_COMPLAINT'], setLemmatizedWordsRefund), axis=1)\n",
    "dataFrameTest2['ESTIMATED_VALUE_COMPLAINED'] = dataFrameTest2.apply(lambda x: getRefundAskedValueInComplaint(x['TEXT_CLEAN_COMPLAINT_WITH_STOP_WORDS'], setLemmatizedWordsRefund), axis=1)\n",
    " \n",
    "    \n",
    "realValuesComplained = taggedDataFrameTest['VALOR_SOLICITADO'].values\n",
    "predictedValuesComplained =dataFrameTest2['ESTIMATED_VALUE_COMPLAINED'].values\n",
    "\n",
    "\n",
    "dataFrameAccuracyValuesComplained= pd.DataFrame(list(zip(predictionsIncludeAskForRefund,realValuesComplained,predictedValuesComplained)), columns = ['ESTIMATED_REFUND_COMPLAINT','REAL_VALUE_COMPLAINED','ESTIMATED_VALUE_COMPLAINED'])\n",
    "\n",
    "dataFrameAccuracyValuesComplained2 = dataFrameAccuracyValuesComplained.loc[dataFrameAccuracyValuesComplained['ESTIMATED_REFUND_COMPLAINT']==1]\n",
    "\n",
    "\n",
    "print('Mean Absolute Error(MAE), Mean Squared Error(MSE) and Median Absolute Deviation(MAD) for Complaints Values predicted over TEST sample of '+str(len(dataFrameAccuracyValuesComplained2)) +' COMPLAINTS: ')\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_squared_error\n",
    "y_true = dataFrameAccuracyValuesComplained2['REAL_VALUE_COMPLAINED'].values\n",
    "y_pred = dataFrameAccuracyValuesComplained2['ESTIMATED_VALUE_COMPLAINED'].values\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print('MAE : %f' %mae)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print('MSE : %f' %mse)\n",
    "mad = median_absolute_error(y_true, y_pred)\n",
    "print('MAD : %f' %mad)\n",
    "\n",
    "dataFrameAccuracyValuesComplained2['ACCURACY'] = dataFrameAccuracyValuesComplained2.apply(lambda x: checkMach(x['REAL_VALUE_COMPLAINED'], x['ESTIMATED_VALUE_COMPLAINED']), axis=1)\n",
    "dataFrameAccuracyValuesComplained2['FALSE_POSITIVE'] = dataFrameAccuracyValuesComplained2.apply(lambda x: checkFalsePositive(x['REAL_VALUE_COMPLAINED'], x['ESTIMATED_VALUE_COMPLAINED']), axis=1)\n",
    "dataFrameAccuracyValuesComplained2['FALSE_NEGATIVE'] = dataFrameAccuracyValuesComplained2.apply(lambda x: checkFalseNegative(x['REAL_VALUE_COMPLAINED'], x['ESTIMATED_VALUE_COMPLAINED']), axis=1)\n",
    "dataFrameAccuracyValuesComplained2['TRUE_POSITIVE'] = dataFrameAccuracyValuesComplained2.apply(lambda x: checkTruePositive(x['REAL_VALUE_COMPLAINED'], x['ESTIMATED_VALUE_COMPLAINED']), axis=1)\n",
    "dataFrameAccuracyValuesComplained2['TRUE_NEGATIVE'] = dataFrameAccuracyValuesComplained2.apply(lambda x: checkTrueNegative(x['REAL_VALUE_COMPLAINED'], x['ESTIMATED_VALUE_COMPLAINED']), axis=1)\n",
    "\n",
    "mean = dataFrameAccuracyValuesComplained2['ACCURACY'].mean()\n",
    "print('ACCURACY Mach : %f' %mean)\n",
    "falsePositive_sum = dataFrameAccuracyValuesComplained2['FALSE_POSITIVE'].sum()\n",
    "falseNegative_sum = dataFrameAccuracyValuesComplained2['FALSE_NEGATIVE'].sum()\n",
    "truePositive_sum = dataFrameAccuracyValuesComplained2['TRUE_POSITIVE'].sum()\n",
    "trueNegative_sum = dataFrameAccuracyValuesComplained2['TRUE_NEGATIVE'].sum()\n",
    "recall = truePositive_sum/(truePositive_sum + falseNegative_sum) #recall = tp / (tp + fn)\n",
    "precision = truePositive_sum/(truePositive_sum + falsePositive_sum)  #precision = tp / (tp + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall) # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('Recall Score : %f' %recall)\n",
    "print('Precision Score : %f' %precision)\n",
    "print('F1 Score : %f' %f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTION THe VALUES FOR THE COMPANIES ANSWERS\n",
    "#now, for the ANSWERS that has been predictec as containing an asking for refund, we will include the values complained\n",
    "dataFrameTest = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/dataFrameTest.csv', sep='\\t')\n",
    "#predictionsCasesRefunded = test_pred\n",
    "\n",
    "dataFrameTest2 = dataFrameTest \n",
    "#including in the test data frame the predictions if is a answer that includes refund\n",
    "dataFrameTest2['REAL_VALUE_REFUNDED'] = taggedDataFrameTest['VALOR_CONCEDIDO'] # the true values\n",
    "#dataFrameTest2['ESTIMATED_REFUND_ANSWER'] = predictionsCasesRefunded\n",
    "dataFrameTest2['ESTIMATED_REFUND_ANSWER'] = taggedDataFrameTest['REEMBOLSO_CONCEDIDO'] # include just the true values, to focus this task just in get the value\n",
    "#in this context, we will use as \"predicted values\" for cases that involves some company refund the true values\n",
    "predictionsCasesRefunded = taggedDataFrameTest['REEMBOLSO_CONCEDIDO']\n",
    "\n",
    "#including in the test data frame the predictions for refund values\n",
    "#dataFrameTest2['ESTIMATED_VALUE_REFUNDED'] = dataFrameTest2.apply(lambda x: getRefundAskedValueInComplaint(x['TEXT_CLEAN_ANSWERS'], setLemmatizedWordsRefundCompany), axis=1)\n",
    "dataFrameTest2['ESTIMATED_VALUE_REFUNDED'] = dataFrameTest2.apply(lambda x: getRefundAskedValueInComplaint(x['TEXT_CLEAN_ANSWERS_WITH_STOP_WORDS'], setLemmatizedWordsRefundCompany), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "realValuesRefunded = taggedDataFrameTest['VALOR_CONCEDIDO'].values\n",
    "predictedValuesRefunded =dataFrameTest2['ESTIMATED_VALUE_REFUNDED'].values\n",
    "\n",
    "\n",
    "#removing lines where there is no preview of asking complaint\n",
    "\n",
    "dataFrameAccuracyValuesRefunded= pd.DataFrame(list(zip(predictionsCasesRefunded,realValuesRefunded,predictedValuesRefunded)), columns = ['ESTIMATED_REFUND_ANSWER','REAL_VALUE_REFUNDED','ESTIMATED_VALUE_REFUNDED'])\n",
    "\n",
    "dataFrameAccuracyValuesRefunded2 = dataFrameAccuracyValuesRefunded.loc[dataFrameAccuracyValuesRefunded['ESTIMATED_REFUND_ANSWER']==1]\n",
    "\n",
    "print('Mean Absolute Error(MAE), Mean Squared Error(MSE) and Median Absolute Deviation(MAD) for REFUNDED VALUES predicted over TEST sample of '+str(len(dataFrameAccuracyValuesRefunded2)) +' ANSWERS: ')\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_squared_error\n",
    "y_true = dataFrameAccuracyValuesRefunded2['REAL_VALUE_REFUNDED'].values\n",
    "y_pred = dataFrameAccuracyValuesRefunded2['ESTIMATED_VALUE_REFUNDED'].values\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print('MAE : %f' %mae)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print('MSE : %f' %mse)\n",
    "mad = median_absolute_error(y_true, y_pred)\n",
    "print('MAD : %f' %mad)\n",
    " \n",
    "dataFrameAccuracyValuesRefunded2['ACCURACY'] = dataFrameAccuracyValuesRefunded2.apply(lambda x: checkMach(x['REAL_VALUE_REFUNDED'], x['ESTIMATED_VALUE_REFUNDED']), axis=1)\n",
    "\n",
    "dataFrameAccuracyValuesRefunded2['FALSE_POSITIVE'] = dataFrameAccuracyValuesRefunded2.apply(lambda x: checkFalsePositive(x['REAL_VALUE_REFUNDED'], x['ESTIMATED_VALUE_REFUNDED']), axis=1)\n",
    "dataFrameAccuracyValuesRefunded2['FALSE_NEGATIVE'] = dataFrameAccuracyValuesRefunded2.apply(lambda x: checkFalseNegative(x['REAL_VALUE_REFUNDED'], x['ESTIMATED_VALUE_REFUNDED']), axis=1)\n",
    "dataFrameAccuracyValuesRefunded2['TRUE_POSITIVE'] = dataFrameAccuracyValuesRefunded2.apply(lambda x: checkTruePositive(x['REAL_VALUE_REFUNDED'], x['ESTIMATED_VALUE_REFUNDED']), axis=1)\n",
    "dataFrameAccuracyValuesRefunded2['TRUE_NEGATIVE'] = dataFrameAccuracyValuesRefunded2.apply(lambda x: checkTrueNegative(x['REAL_VALUE_REFUNDED'], x['ESTIMATED_VALUE_REFUNDED']), axis=1)\n",
    "\n",
    "mean = dataFrameAccuracyValuesRefunded2['ACCURACY'].mean()\n",
    "print('ACCURACY Mach : %f' %mean)\n",
    "\n",
    "falsePositive_sum = dataFrameAccuracyValuesRefunded2['FALSE_POSITIVE'].sum()\n",
    "falseNegative_sum = dataFrameAccuracyValuesRefunded2['FALSE_NEGATIVE'].sum()\n",
    "truePositive_sum = dataFrameAccuracyValuesRefunded2['TRUE_POSITIVE'].sum()\n",
    "trueNegative_sum = dataFrameAccuracyValuesRefunded2['TRUE_NEGATIVE'].sum()\n",
    "recall = truePositive_sum/(truePositive_sum + falseNegative_sum) #recall = tp / (tp + fn)\n",
    "precision = truePositive_sum/(truePositive_sum + falsePositive_sum)  #precision = tp / (tp + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall) # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print('Recall Score : %f' %recall)\n",
    "print('Precision Score : %f' %precision)\n",
    "print('F1 Score : %f' %f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part-of-Speech Tagging\n",
    "#using the MAC-MORPHO Brazilian Portuguese POS-tagged\n",
    "nltk.corpus.mac_morpho.words()\n",
    "#['Jersei', 'atinge', 'm\\xe9dia', 'de', 'Cr$', '1,4', ...]\n",
    "nltk.corpus.mac_morpho.sents() \n",
    "#[['Jersei', 'atinge', 'm\\xe9dia', 'de', 'Cr$', '1,4', 'milh\\xe3o',\n",
    "#'em', 'a', 'venda', 'de', 'a', 'Pinhal', 'em', 'S\\xe3o', 'Paulo'],\n",
    "#['Programe', 'sua', 'viagem', 'a', 'a', 'Exposi\\xe7\\xe3o', 'Nacional',\n",
    "#'do', 'Zebu', ',', 'que', 'come\\xe7a', 'dia', '25'], ...]\n",
    "nltk.corpus.mac_morpho.tagged_words()\n",
    "#[('Jersei', 'N'), ('atinge', 'V'), ('m\\xe9dia', 'N'), ...]\n",
    "nltk.corpus.mac_morpho.tagged_sents()\n",
    "\n",
    "#lets contruct our 3-gram tagger from MAC_MORPHO Corpus\n",
    "corpusSentences = nltk.corpus.mac_morpho.tagged_sents()\n",
    "#Lets divide into train and test datasets\n",
    "size = int(len(corpusSentences) * 0.9)\n",
    "size\n",
    "train_sentences = corpusSentences[:size]\n",
    "test_sentences = corpusSentences[size:]\n",
    "\n",
    "tagger0 = nltk.DefaultTagger('N')\n",
    "tagger1 = nltk.UnigramTagger(train_sentences, backoff=tagger0)\n",
    "#tagger1.evaluate(test_sentences)#evaluating the accuracy over test dataset\n",
    "tagger2 = nltk.BigramTagger(train_sentences, backoff=tagger1)\n",
    "#tagger2.evaluate(test_sentences)#evaluating the accuracy over test dataset\n",
    "tagger3 = nltk.TrigramTagger(train_sentences, backoff=tagger2)\n",
    "tagger3.evaluate(test_sentences)#evaluating the accuracy over test dataset\n",
    "\n",
    "\n",
    "print(fraseTeste)\n",
    "tags = tagger3.tag(fraseTeste)\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NER (Named entity recognition) to identify values in complaints\n",
    "\n",
    "chunksEntities = nltk.ne_chunk(tags, binary=True)\n",
    "#namedEntity = nltk.ne_chunk(tags)\n",
    "\n",
    "#chunksEntities.draw()\n",
    "#print(chunksEntities)\n",
    "\n",
    "\n",
    "def returnAllLabels(chunks):\n",
    "    labels = []\n",
    "    for chunkEntity in chunks:\n",
    "        labels.append(chunkEntity[1])\n",
    "    return labels\n",
    "    \n",
    "\n",
    "#function to return the value is been asked for refund or refunded\n",
    "def getRefundAskedValue(complaint):\n",
    "    tokensComplaint = Tokenize(complaint)\n",
    "    tags = tagger3.tag(tokensComplaint)\n",
    "    chunksEntities = nltk.ne_chunk(tags, binary=True)\n",
    "    \n",
    "    labels = returnAllLabels(chunksEntities)\n",
    "    print(labels)\n",
    "    #print(len(labels))\n",
    "    iterator = 0\n",
    "    while(iterator < len(labels)-1 ) : #the last one label will not be counted.\n",
    "            #print(labels[iterator] + labels[iterator+1])\n",
    "            if labels[iterator] == '$' and (labels[iterator+1] == 'NUM'): #search for values preceeded by '$\n",
    "                entity = chunksEntities[iterator+1]\n",
    "                entityValue = entity[0]\n",
    "                print(\"CHUNKED Monetary Value: \" + entityValue)#return the monetary value finded\n",
    "                return entityValue ##retur\n",
    "            iterator += 1 \n",
    "    return 0\n",
    "\n",
    "def convertStringBrazilCurrencyToFloat(valueString):\n",
    "    strReplaced = valueString.replace(\".\",\"\") #removing thousand brazilian separator\n",
    "    strReplaced = strReplaced.replace(\",\",\".\") #replacing brazilian decimal separator for decimal separator\n",
    "    try:\n",
    "        return float(strReplaced)\n",
    "    except:\n",
    "        pass   \n",
    "    return 0.0 #return 0 if there is some error in casting\n",
    "\n",
    "WINDOW_SIZE_CONSTANT =5 #the window size(radio counting from he value) to search for special words\n",
    "WINDOW_SIZE_CONSTANT_WEIGHT = 3 #the widows size (radio counting from the value) to observe a word that points the value signal \n",
    "\n",
    "#function to return the value is been asked for refund\n",
    "def getRefundAskedValueInComplaint(complaintLematized,setLemmatizedSpecialWords):\n",
    "    valuesFounded = [0] #list that contains tha asked values founded in the complaints\n",
    "    \n",
    "    tokensComplaint = Tokenize(complaintLematized)\n",
    "    tags = tagger3.tag(tokensComplaint)\n",
    "    chunksEntities = nltk.ne_chunk(tags, binary=True)\n",
    "    \n",
    "    labels = returnAllLabels(chunksEntities)\n",
    "    #print(len(labels))\n",
    "    iterator = 0\n",
    "    while(iterator < len(labels)-1 ) : #the last one label will not be counted.\n",
    "            #print(labels[iterator] + labels[iterator+1])\n",
    "            \n",
    "            if labels[iterator] == '$' and ((labels[iterator+1] == 'NUM') or (labels[iterator+1] == 'N')): #search for values preceeded by '$\n",
    "                entity = chunksEntities[iterator+1]\n",
    "                entityValue = entity[0]\n",
    "                #print(\"CHUNKED Monetary Value: \" + entityValue)#return the monetary value finded\n",
    "                #if checkSpecialLemasInWindow(tokensComplaint, iterator, setLemmatizedSpecialWords, WINDOW_SIZE_CONSTANT):valuesFounded.append(convertStringBrazilCurrencyToFloat(entityValue))  ##adding the value if there is some special word close it\n",
    "                \n",
    "                ##code in case take in count the weight of the value\n",
    "                if checkSpecialLemasInWindow(tokensComplaint, iterator, setLemmatizedSpecialWords, WINDOW_SIZE_CONSTANT):\n",
    "                    valueFounded = convertStringBrazilCurrencyToFloat(entityValue)\n",
    "                    semanticValue = returnSemanticValue(tokensComplaint, iterator, valueFounded, WINDOW_SIZE_CONSTANT_WEIGHT)\n",
    "                    valuesFounded.append(semanticValue)  #adding the value if there is some special word close it\n",
    "                    #print(\"value: \"+ str(valueFounded))\n",
    "                    #print(\"Semantic value: \"+ str(semanticValue))\n",
    "            \n",
    "            iterator += 1 \n",
    "   \n",
    "    return sum(valuesFounded) #return the sum of the values founded\n",
    "\n",
    "#function that, based on the position of a monetary value, return if there is a 'special lema' in the window of size n\n",
    "def checkSpecialLemasInWindow(tokensLemmatized, positionValueToCheck, setSpecialLemas, sizeWindow):\n",
    "    sizeText = len(tokensLemmatized)\n",
    "    #firts, lets check for the lemas before the position\n",
    "    pointer = positionValueToCheck\n",
    "    while( (pointer != 0) and (positionValueToCheck-pointer <= sizeWindow) ):\n",
    "        lemaToCheck = tokensLemmatized[pointer]\n",
    "        if lemaToCheck in setSpecialLemas:\n",
    "         \n",
    "            return True\n",
    "        pointer -= 1 #decrement the value of the pointer\n",
    "        \n",
    "    #now, lets check for the lemas after the position\n",
    "    pointer = positionValueToCheck\n",
    "    while( (pointer < sizeText) and (pointer-positionValueToCheck <= sizeWindow) ):\n",
    "        lemaToCheck = tokensLemmatized[pointer]\n",
    "        if lemaToCheck in setSpecialLemas:\n",
    "           \n",
    "            return True\n",
    "        pointer += 1 #increment the value of the pointer\n",
    "    \n",
    "    return False #return false if there is no one special lema in the window\n",
    "\n",
    "def returnSemanticValue(tokensLemmatized, positionValueToCheck, value, sizeWindow):\n",
    "    sizeText = len(tokensLemmatized)\n",
    "\n",
    "    #firts, lets check for the tokens before the position\n",
    "    pointer = positionValueToCheck\n",
    "    while( (pointer != 0) and (positionValueToCheck-pointer <= sizeWindow) ):\n",
    "        lemaToCheck = tokensLemmatized[pointer]\n",
    "        if lemaToCheck in setLemmasPointSubtraction: ##case subtraction\n",
    "            return (value * -1)\n",
    "        pointer -= 1 #decresing the value of the pointer\n",
    "   \n",
    "    return value;#return just the value in case not identify subtraction\n",
    "\n",
    "    #now, lets check for the lemas after the position\n",
    "    #pointer = positionValueToCheck\n",
    "    #while( (pointer < sizeText) and (pointer-positionValueToCheck <= sizeWindow) ):\n",
    "    #    lemaToCheck = tokensLemmatized[pointer]\n",
    "    #    if lemaToCheck in setSpecialLemas:\n",
    "     #       #print(\"Lemma checked After: \" + lemaToCheck) \n",
    "      #      return True\n",
    "       # pointer += 1 #increasing the value of the pointer\n",
    "    \n",
    "    #return False #return false if there is no one special lema in the window\n",
    "\n",
    "\n",
    "exampleNoComplaint = 'Quero apenas reclamar dos serviços da operadora, que são muito mais lentos do que eu imaginava.'\n",
    "exampleComplaint2 = 'Sou titular do telefone (61) 98202-1103. Estou muito chateado com o valor cobrado indevidamente de R$ 450,28 na minha conta. Quero reembolso!'\n",
    "exampleComplaint3 = \"Desejo cancelar minha assinatura, e com isenção da multa de R$ 47,84. \"\n",
    "exampleAnswer1 = \"corrigimos a fatura com vencimento 25/05/2020  de R$145,83 para R$118,98\"\n",
    "exampleAnswer2 = \"seria isenta a multa rescisória, onde a mesma não será cobrada e realizamos a isenção das faturas de R$69,87 e R$112,18\"\n",
    "exampleAnswer3 = \"vamos devoler a diferença  de R$145,83 para R$118,98\"\n",
    "\n",
    "#getRefundAskedValue(preprocess_text(exampleComplaint))\n",
    "#print(preprocess_text(exampleComplaint2))\n",
    "#getRefundAskedValueInComplaint(preprocess_text(exampleComplaint2,False), setLemmatizedWordsRefund)\n",
    "getRefundAskedValueInComplaint(preprocess_text(exampleAnswer3,True), setLemmatizedWordsRefundCompany)\n",
    "#preprocess_text(exampleAnswer1,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTING THE refunded values by the companies\n",
    "dataFrameRefundAgree = dataFrame.loc[dataFrame['REFUND_AGREE'] == 1]\n",
    "dataFrameRefundAgree['ESTIMATED_VALUE_REFUNDED'] = dataFrameRefundAgree['TEXT_CLEAN_ANSWERS'].apply(getRefundAskedValue) \n",
    "\n",
    "#now let`s analise the accuracy of the refund value prediction just to lines where the prediction was 1\n",
    "dataFrameTestAccuracyValues = dataFrameRefundAgree.loc[dataFrameRefundAgree['REFUND_AGREE']==1]\n",
    "dataFrameTestAccuracyValues.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameRefundAgree[['REAL_VALUE_REFUNDED','ESTIMATED_VALUE_REFUNDED']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'The company has R$ 50,00 in debit with me!'\n",
    "\n",
    "n = 3\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, let`s analyze the accuracy of 1200 predictinos using test sample\n",
    "dataFrameTest = dataFrameTest1200Vectors\n",
    "testLabels = dataFrameTest.loc[:, ['INCLUDE_REFUND']]\n",
    "dfTest = dataFrameTest.drop(['INCLUDE_REFUND'], axis=1)\n",
    "\n",
    "test_x = dfTest\n",
    "test_y = testLabels \n",
    "test_pred = model.predict(test_x)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print('Accuracy, Confusion Matrix and F1 Score for the AdaBoostClassifier in Test sample of 1200 complaints.  ')\n",
    "print(accuracy_score(test_y, test_pred))\n",
    "print(confusion_matrix(test_y,test_pred))\n",
    "f1 = sklearn.metrics.f1_score(test_y, test_pred)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets trains the classifer with the vectorized companie answers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(docs_vectors.drop('REFUND_AGREE', axis = 1),\n",
    "                                                   docs_vectors['REFUND_AGREE'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=800, random_state = 1)\n",
    "model.fit(train_x, train_y)\n",
    "test_pred = model.predict(test_x)\n",
    "\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the AdaBoostClassifier in TRAIN sample of '+str(len(dataFrame)) +' ANSWERS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y,test_pred))\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(test_y,test_pred)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred)\n",
    "print('Precision score: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we apply the classification model over the test sample\n",
    "docs_vectorsTest = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors400TestAnswers.csv', sep='\\t')\n",
    "test_x = docs_vectorsTest\n",
    "test_y = taggedDataFrameTest['REEMBOLSO_CONCEDIDO'].values\n",
    "test_pred = model.predict(test_x)\n",
    "\n",
    "\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the AdaBoostClassifier in TEST sample of '+str(len(test_x)) +' ANSWERS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y,test_pred))\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(test_y,test_pred)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred)\n",
    "print('Precision score: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertStringBrazilCurrencyToFloat(valueString):\n",
    "    strReplaced = valueString.replace(\".\",\"\") #removing thousand brazilian separator\n",
    "    strReplaced = strReplaced.replace(\",\",\".\") #replacing brazilian decimal separator for decimal separator\n",
    "    try:\n",
    "        return float(strReplaced)\n",
    "    except:\n",
    "        pass   \n",
    "    return 0.0\n",
    "\n",
    "valor = \"3.151,70\"\n",
    "convertStringBrazilCurrencyToFloat(valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textExemplePreProcess= 'No dia 18 de abril de 2020, realizei a abertura de um protocolo junto a TIM para verificar o \"sumiço\" de cerca de R$7,00 reais de crédito que desapareceram de minha linha telefônica (62) 98215-6179. No meu histórico de consumo no site MEUTIM não consta data, hora ou motivo do débito indevido. Em conversa com atendente, ele também não soube me explicar o motivo do desaparecimento do saldo da minha conta e realizou a abertura do protocolo número 2020403961314. Dias depois o protocolo foi dado como finalizado sem a companhia fazer o ressarcimento do valor ou sequer me dar um parecer a respeito do ocorrido. Solicito, a devolução de meus crédito ou, pelo menos, uma explicação sobre o fato.'\n",
    "preprocess_text(textExemplePreProcess,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the classifiers to make the predictiins over CUSTOMER COMPLAINTS\n",
    "\n",
    "dataFrameTrain = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors1200Complaints.csv', sep='\\t')\n",
    "\n",
    "features = dataFrameTrain.columns.difference(['INCLUDE_REFUND'])\n",
    "\n",
    "#print(features)\n",
    "\n",
    "#lets point the x as the features to be used, and Y as the targetVariable(INCLUDE_REFUND)\n",
    "X = dataFrameTrain[features].values\n",
    "y = dataFrameTrain['INCLUDE_REFUND'].values\n",
    "\n",
    "\n",
    "#lets train an ADABooster model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataFrameTrain.drop('INCLUDE_REFUND', axis = 1),\n",
    "                                                   dataFrameTrain['INCLUDE_REFUND'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "model_ADAB = AdaBoostClassifier(n_estimators=800, random_state = 1)\n",
    "model_ADAB.fit(train_x, train_y)\n",
    "scores_ADAB = cross_val_score(model_ADAB, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_ADAB.mean())\n",
    "\n",
    "\n",
    "#lets train a DECISION TREE model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier_dt = DecisionTreeClassifier(random_state=1981, criterion='gini', max_depth=10)\n",
    "classifier_dt.fit(X, y)#here Decicion Tree model is fitted differently from the others, \n",
    "\n",
    "#aplying cross validation to the Decision Tree model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_dt = cross_val_score(classifier_dt, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_dt.mean())\n",
    "\n",
    "\n",
    "#Constructing a random forest classfier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_rf =    RandomForestClassifier(random_state=1981, criterion='gini', max_depth=10, n_estimators=800, n_jobs=-1)\n",
    "classifier_rf.fit(train_x, train_y) \n",
    "\n",
    "#aplying cross validation to the Ramdom Forest model\n",
    "scores_rf = cross_val_score(classifier_rf, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_rf.mean())\n",
    "\n",
    "#analising the festure importance in the random forest model\n",
    "#features_importance = zip(classifier_rf.feature_importances_, features)\n",
    "#for importance, feature in sorted(features_importance, reverse=True):\n",
    "#    print(\"%s: %f%%\" % (feature, importance*100))\n",
    "    \n",
    "#Constructing a GradientBoosting classfier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier_GB = GradientBoostingClassifier(n_estimators=800, random_state = 1)\n",
    "classifier_GB.fit(train_x, train_y)\n",
    "#applying cross validation\n",
    "scores_GB = cross_val_score(classifier_GB, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_GB.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here let's use the TEST dataset to make the predictions over CUSTOMER COMPLAINTS using different classifiers\n",
    "\n",
    "docs_vectorsTest = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors400TestComplaints.csv', sep='\\t')\n",
    "test_x = docs_vectorsTest\n",
    "test_y = taggedDataFrameTest['REEMBOLSO_SOLICITADO'].values #test labels\n",
    "\n",
    "#ADA_BOOSTING\n",
    "test_pred_ADAB = model_ADAB.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the AdaBoostClassifier in TEST sample of '+str(len(test_x)) +' complaints: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_ADAB)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y,test_pred_ADAB))\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(test_y,test_pred_ADAB)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_ADAB)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_ADAB)\n",
    "print('Precision score: %f' % precision)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#DECISION TREE\n",
    "test_pred_DTree = classifier_dt.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the DecisionTree_Classifier in TEST sample of '+str(len(test_x)) +' COMPLAINTS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_DTree)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y, test_pred_DTree))\n",
    "f1 = sklearn.metrics.f1_score(test_y, test_pred_DTree)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_DTree)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_DTree)\n",
    "print('Precision score: %f' % precision)\n",
    "\n",
    "print('\\n')\n",
    "#RANDOM FOREST\n",
    "test_pred_RandForst = classifier_rf.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the RandomForest_Classifier in TEST sample of '+str(len(test_x)) +' COMPLAINTS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_RandForst)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y, test_pred_RandForst))\n",
    "f1 = sklearn.metrics.f1_score(test_y, test_pred_RandForst)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_RandForst)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_RandForst)\n",
    "print('Precision score: %f' % precision)\n",
    "\n",
    "print('\\n')\n",
    "#GRADIENT BOOSTING\n",
    "classifier_GB\n",
    "test_pred_GB = classifier_GB.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the GradientBoosting_Classifier in TEST sample of '+str(len(test_x)) +' COMPLAINTS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_GB)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y, test_pred_GB))\n",
    "f1 = sklearn.metrics.f1_score(test_y, test_pred_GB)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_GB)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_GB)\n",
    "print('Precision score: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the classifiers to make the predictiins over COMPANY ANSWERS\n",
    "\n",
    "dataFrameTrain = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors1200Answers.csv', sep='\\t')\n",
    "\n",
    "features = dataFrameTrain.columns.difference(['REFUND_AGREE'])\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#print(features)\n",
    "\n",
    "#lets point the x as the features to be used, and Y as the targetVariable(REFUND_AGREE)\n",
    "X = dataFrameTrain[features].values\n",
    "y = dataFrameTrain['REFUND_AGREE'].values\n",
    "\n",
    "\n",
    "#lets train an ADABooster model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataFrameTrain.drop('REFUND_AGREE', axis = 1),\n",
    "                                                   dataFrameTrain['REFUND_AGREE'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "model_ADAB = AdaBoostClassifier(n_estimators=800, random_state = 1)\n",
    "model_ADAB.fit(train_x, train_y)\n",
    "scores_ADAB = cross_val_score(model_ADAB, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_ADAB.mean())\n",
    "\n",
    "\n",
    "#lets train a DECISION TREE model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier_dt = DecisionTreeClassifier(random_state=1981, criterion='gini', max_depth=10)\n",
    "classifier_dt.fit(train_x, train_y)#here Decicion Tree model is fitted differently from the others, \n",
    "\n",
    "#aplying cross validation to the Decision Tree model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_dt = cross_val_score(classifier_dt, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_dt.mean())\n",
    "\n",
    "\n",
    "#Constructing a random forest classfier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_rf =    RandomForestClassifier(random_state=1981, criterion='gini', max_depth=10, n_estimators=800, n_jobs=-1)\n",
    "classifier_rf.fit(train_x, train_y) \n",
    "\n",
    "#aplying cross validation to the Ramdom Forest model\n",
    "scores_rf = cross_val_score(classifier_rf, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_rf.mean())\n",
    "\n",
    "#analising the festure importance in the random forest model\n",
    "#features_importance = zip(classifier_rf.feature_importances_, features)\n",
    "#for importance, feature in sorted(features_importance, reverse=True):\n",
    "#    print(\"%s: %f%%\" % (feature, importance*100))\n",
    "    \n",
    "#Constructing a GradientBoosting classfier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier_GB = GradientBoostingClassifier(n_estimators=800, random_state = 1)\n",
    "classifier_GB.fit(train_x, train_y)\n",
    "#applying cross validation\n",
    "scores_GB = cross_val_score(classifier_GB, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_GB.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here let's use the TEST dataset to make the predictions over COMPANIES ANSWERS\n",
    "\n",
    "docs_vectorsTest = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors400TestAnswers.csv', sep='\\t')\n",
    "test_x = docs_vectorsTest\n",
    "test_y = taggedDataFrameTest['REEMBOLSO_CONCEDIDO'].values\n",
    "\n",
    "#ADA_BOOSTING\n",
    "test_pred_ADAB = model_ADAB.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the AdaBoostClassifier in TEST sample of '+str(len(test_x)) +' ANSWERS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_ADAB)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y,test_pred_ADAB))\n",
    "f1 = sklearn.metrics.f1_score(test_y,test_pred_ADAB)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_ADAB)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_ADAB)\n",
    "print('Precision score: %f' % precision)\n",
    "print('\\n')\n",
    "\n",
    "#DECISION TREE\n",
    "test_pred_DTree = classifier_dt.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the DecisionTree_Classifier in TEST sample of '+str(len(test_x)) +' ANSWERS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_DTree)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y, test_pred_DTree))\n",
    "f1 = sklearn.metrics.f1_score(test_y, test_pred_DTree)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_DTree)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_DTree)\n",
    "print('Precision score: %f' % precision)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#RANDOM FOREST\n",
    "test_pred_RandForst = classifier_rf.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the RandomForest_Classifier in TEST sample of '+str(len(test_x)) +' ANSWERS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_RandForst)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y, test_pred_RandForst))\n",
    "f1 = sklearn.metrics.f1_score(test_y, test_pred_RandForst)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_RandForst)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_RandForst)\n",
    "print('Precision score: %f' % precision)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#GRADIENT BOOSTING\n",
    "test_pred_GB = classifier_GB.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the GradientBoosting_Classifier in TEST sample of '+str(len(test_x)) +' ANSWERS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_GB)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y, test_pred_GB))\n",
    "f1 = sklearn.metrics.f1_score(test_y, test_pred_GB)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_GB)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_GB)\n",
    "print('Precision score: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameTrain = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors1200Answers.csv', sep='\\t')\n",
    "dataFrameTrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"corrigimos a fatura com vencimento 25/05/2020  de R$145,83 para R$118,98\"\n",
    "#answer = \"seria isenta a multa rescisória, onde a mesma não será cobrada e realizamos a isenção das faturas de R$69,87 e R$112,18\"\n",
    "cleanedAnswer = preprocess_text(answer,False)\n",
    "value = getRefundAskedValueInComplaint(cleanedAnswer, setLemmatizedWordsRefundCompany)\n",
    "print(answer)\n",
    "print(cleanedAnswer)\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataFrameTrain = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors1200Answers.csv', sep='\\t')\n",
    "     \n",
    "features = dataFrameTrain.columns.difference(['REFUND_AGREE'])\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#print(features)\n",
    "\n",
    "#lets point the x as the features to be used, and Y as the targetVariable(REFUND_AGREE)\n",
    "X = dataFrameTrain[features].values\n",
    "y = dataFrameTrain['REFUND_AGREE'].values\n",
    "\n",
    "\n",
    "#lets train an ADABooster model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataFrameTrain.drop('REFUND_AGREE', axis = 1),\n",
    "                                                   dataFrameTrain['REFUND_AGREE'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "model_ADAB = AdaBoostClassifier(n_estimators=800, random_state = 1)\n",
    "model_ADAB.fit(train_x, train_y)\n",
    "scores_ADAB = cross_val_score(model_ADAB, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_ADAB.mean())\n",
    "\n",
    "\n",
    "#lets train a DECISION TREE model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier_dt = DecisionTreeClassifier(random_state=1981, criterion='gini', max_depth=10)\n",
    "classifier_dt.fit(X, y)\n",
    "\n",
    "#aplying cross validation to the Decision Tree model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_dt = cross_val_score(classifier_dt, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_dt.mean())\n",
    "\n",
    "\n",
    "#Constructing a random forest classfier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_rf =    RandomForestClassifier(random_state=1981, criterion='gini', max_depth=10, n_estimators=50, n_jobs=-1)\n",
    "classifier_rf.fit(X, y) \n",
    "\n",
    "#aplying cross validation to the Ramdom Forest model\n",
    "scores_rf = cross_val_score(classifier_rf, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_rf.mean())\n",
    "\n",
    "#analising the festure importance in the random forest model\n",
    "#features_importance = zip(classifier_rf.feature_importances_, features)\n",
    "#for importance, feature in sorted(features_importance, reverse=True):\n",
    "#    print(\"%s: %f%%\" % (feature, importance*100))\n",
    "    \n",
    "#Constructing a GradientBoosting classfier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier_GB = GradientBoostingClassifier(n_estimators=800, random_state = 1)\n",
    "classifier_GB.fit(X, y)\n",
    "#applying cross validation\n",
    "scores_GB = cross_val_score(classifier_GB, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_GB.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataFrameTrain = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors1200Answers.csv', sep='\\t')\n",
    "\n",
    "features = dataFrameTrain.columns.difference(['REFUND_AGREE'])\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#print(features)\n",
    "\n",
    "#lets point the x as the features to be used, and Y as the targetVariable(REFUND_AGREE)\n",
    "X = dataFrameTrain[features].values\n",
    "y = dataFrameTrain['REFUND_AGREE'].values\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataFrameTrain.drop('REFUND_AGREE', axis = 1),\n",
    "                                                   dataFrameTrain['REFUND_AGREE'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "#model_ADAB.fit(train_x, train_y)\n",
    "\n",
    "#lets train a DECISION TREE model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier_dt = DecisionTreeClassifier(random_state=1981, criterion='gini', max_depth=15)\n",
    "classifier_dt.fit(train_x, train_y)\n",
    "\n",
    "#aplying cross validation to the Decision Tree model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_dt = cross_val_score(classifier_dt, X, y, scoring='accuracy', cv=5)\n",
    "print(scores_dt.mean())\n",
    "\n",
    "\n",
    "docs_vectorsTest = pd.read_csv(r'C:/Users/leandro.lira/Desktop/arquivos dissertacao Leandro/docsVectors400TestAnswers.csv', sep='\\t')\n",
    "test_x = docs_vectorsTest\n",
    "test_y = taggedDataFrameTest['REEMBOLSO_CONCEDIDO'].values\n",
    "\n",
    "#GRADIENT BOOSTING\n",
    "test_pred_GB = classifier_dt.predict(test_x)\n",
    "print('Accuracy, Confusion Matrix, F1 Score, Recall and Precision for the  in TEST sample of '+str(len(test_x)) +' ANSWERS: ')\n",
    "accuracy = accuracy_score(test_y, test_pred_GB)\n",
    "print('Accuracy score: %f' % accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_y, test_pred_GB))\n",
    "f1 = sklearn.metrics.f1_score(test_y, test_pred_GB)\n",
    "print('F1 score: %f' % f1)\n",
    "recall = recall_score(test_y,test_pred_GB)\n",
    "print('Recall score: %f' % recall)\n",
    "precision = precision_score(test_y,test_pred_GB)\n",
    "print('Precision score: %f' % precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
